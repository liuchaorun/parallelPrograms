# Adaptive Optimization for Petascale Heterogeneous CPU/GPU Computing

## 摘要

在本文中，我们描述了为天河1开发Linpack基准测试的实验，该系统是Petascale（千万亿次） CPU/GPU超级计算机系统，这是迄今为止尝试过的最大的GPU加速系统。本文提出了一种自适应优化框架，以平衡GPU和CPU上的工作负载分配与可忽略的运行时开销，从而获得比静态或训练分区方法更好的性能。通过软件流水线技术可以有效地隐藏CPU-GPU通信开销，这对于大型内存绑定应用程序特别有用。结合其他传统优化，我们使用自适应优化框架进行优化的Linpack在TianHe-1的单个计算元素上实现了196.7GFLOPS。此结果是峰值计算能力的70.1％，比使用供应商的库的结果快3.3倍。在TianHe-1的完整配置上，我们的优化使Linpack的性能达到0.563 PFLOPS，这使TianHe-1成为2009年11月发布的Top500排行榜中排名第五的超级计算机。

## 介绍

超级计算机进入千万亿级领域。

在这种异构（或混合）系统中，CPU在广泛的应用程序中提供通用性，而专用的加速器为特定的计算模式提供更好的电源效率和每美元性能。

由于GPU的能力和可编程性迅速提高，因此它们不仅被用作图形加速器，而且还被用作高度并行的可编程处理器。

本文介绍了我们开发Linpack基准以实现在TianHe-1上达到此性能水平的经验，这是GPU加速混合系统从未尝试过的规模，使TianHe-1成为2009年11月Top500排行榜中排名第五的超级计算机[1]。

我们的优化方法将为开发用于大型CPU+GPU异构集群系统的高性能应用程序的更一般领域提供有价值的见解。

主要问题

CPU内核和GPU之间的工作负载不平衡以及CPU和GPU之间的低带宽通信是性能的两个主要障碍。

本文介绍了在为千万亿级TianHe-1系统制作Linpack版本时如何解决这两个障碍。

本文的实现中，工作负载可以自适应地分配到CPU内核和GPU，而运行时开销却可以忽略不计，与静态分区方法相比，可以实现更好的负载平衡。

静态方法导致工作负载不平衡，并且累积的执行时间更长，而通过训练/分析来改进静态方法会消耗过多的功率，无法在千万亿级超算环境中实际使用。

软件流水技术

因此，我们的框架包括应用（1）一种新的自适应工作分区技术，以在单个CPUGPU计算元素中的CPU内核和GPU之间动态改善TianHe-1中异构性的负载平衡，以及（2）一种新的软件流水线 GPU计算与内核执行和数据传输重叠的技术。 我们的优化框架是自适应的，因为我们在运行时自动将工作负载分配给CPU内核和GPU时执行了所有优化。

可以被用作重复性多的高性能计算软件提速

此外，在编排CPU和GPU存储器之间的数据移动时必须格外注意，以使内核执行和数据传输可以重叠。

本文的贡献：
1. 我们提出了一种自适应分区技术，可将程序中的计算分布在CPU / GPU混合系统中的CPU内核和GPU之间，以实现平衡的工作负载，并且运行时开销可忽略不计。

2. 我们提出了一种用于GPU计算的软件流水线技术，以通过与内核执行重叠来有效隐藏CPU和GPU内存之间的通信开销。

3. 我们实现了Linpack的一个版本，该版本是通过将著名的优化技术与上述两个针对TianHe-1的新技术相结合而开发的。 最终的实现提供了0.563 PFLOPS的Linpack性能，使TianHe-1成为2009年11月发布的Top500排行榜中排名第五的超级计算机。此外，一个CPUGPU计算单元实现的效率代表其峰值性能的70.1％。

## 相关工作

Ryoo[14]提出了优化内存访问的一般原则，在CUDA架构中对全局内存访问进行了重新排序，以将对相同或相邻内存位置的请求进行组合。

Gregorio[15]使用四个GPU来加速FLAME编程系统中的矩阵矩阵乘积和Cholesky因式分解操作。

Qilin异构编程系统提出了自适应映射，这是一种将计算映射到CPU + GPU机器上的处理元素的自动技术。

要找到接近最佳的映射，麒麟中的程序首先需要进行训练，并且在运行时不对映射进行调整，由于不可避免的性能波动以及时间和精力的浪费，使得麒麟不适用于大型系统。

Roadrunner 8GB内存大于GPU 1-2GB内存

主机和加速器之间的通信开销

## 天河1号

## 两级自适应任务映射

动机

由于存在cpu cores之间和cpu gpu之间的平衡性

方法

该方法的主要思想是，我们在运行时测量GPU和CPU的性能GFLOPS，并用它来指导下一个工作负载的划分。

第一级任务映射是CPU和GPU之间的，主要由以下两个步骤：

1. 计算$W_{G}=W*GSplit W_{C}=W*(1-GSplit)$
2. 计算新的GSplit

第二级任务映射一个CPU内部的CPU cores之间的

1. The workload of core i is $W_{C} ∗ CSplit_i$
2. 计算新的CSplit

DGEMM的实现

## 软件流水线

动机

数据首先复制到PCI-E内存，然后再传输到GPU本地内存。

PCIE2.0 4-8 GBps

CPU host memory and PCIE2.0 MBps

我们提出了一种软件流水线方法，该方法可以使内核执行和CPU和GPU之间针对大型内存绑定应用程序的数据传输重叠。

划分
1. 从主机到GPU本地内存的数据复制
2. 内核执行
3. 结果输出
   
我们的方法将任务之间的三个阶段进行流水线处理，以使数据传输与内核执行重叠。

方法：

类似于流水线方法，分为三个阶段input exex output

DGEMM的实现

bounce corner turn

## 评估

方法

实验的软件环境，相关的软件版本号

单计算元素结果

多计算元素结果

## 结论

CPU和GPU之间的工作负载不平衡以及CPU和GPU之间的数据传输是GPU计算性能的两个主要障碍。 我们已经描述了在为Petascale混合系统（称为TianHe-1）开发Linpack的实现时解决这两个障碍的经验，该系统将传统的x86-64主机处理器与AMD HD4870×2 GPU结合在一起。 在我们的实现中，使用自适应分区技术以可忽略的运行时开销将工作负载自适应地分配到CPU内核和GPU，与静态分区方法相比，可以实现更好的负载平衡。我们的软件流水线技术可以有效地隐藏CPU-GPU的通信开销，这对于大型内存绑定应用程序尤其有用。

在单个TianHe-1计算元素上，我们的自适应优化框架将矩阵矩阵乘法（DGEMM）的性能提高了22.19％。我们实施的Linpack达到了196.7 GFLOPS，是峰值计算能力的70.1％，比使用供应商库的结果快3.3倍。在多计算元素配置上，Linpack具有良好的可伸缩性，在一个机柜上可实现8.02 TFLOPS，在完整的80机柜TianHe-1配置上可实现563.1 TFLOPS。Linpack的实施使TianHe-1成为2009年11月发布的Top500排行榜中排名第五的超级计算机。